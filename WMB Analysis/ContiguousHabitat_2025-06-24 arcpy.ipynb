{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'duckdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mduckdb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m listlayers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'duckdb'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import duckdb\n",
    "import geopandas as gpd\n",
    "from fiona import listlayers\n",
    "%load_ext sql\n",
    "\n",
    "import logging\n",
    "#set up logging \n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "debug=logging.debug\n",
    "info=logging.info\n",
    "warning=logging.warning\n",
    "error=logging.error\n",
    "\n",
    "tempdir = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\MarxanInputs\\Contiguous'\n",
    "gdb = os.path.join(tempdir, \"ContiguousHabitat_2025-06-25.gdb\")\n",
    "tempgdb = os.path.join(tempdir, \"TEMP_ContiguousHabitat.gdb\")\n",
    "\n",
    "studyArea = r\"\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\WMB_Study_Area_2024_07_30\\WMB_Study_Area_2024_07_30.shp\"\n",
    "privateLand = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\ownership\\iflb_own_studyArea.shp'\n",
    "vri_gdb = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\Disturbance products for BRFN - Copy\\data\\BRFN_FOREST_DSTRB.gdb'\n",
    "wmb = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Project\\WMB_Planning.gdb\\StudyArea_Erase' # Note that this data is the study area split by the closest WMB\n",
    "\n",
    "disturbance_dir = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\Disturbance'\n",
    "\n",
    "workspace = r'C:\\Users\\NROSS\\OneDrive - Government of BC\\Documents\\Projects\\WMBTesting\\wmb-duckdb'\n",
    "work_db = os.path.join(workspace, \"contiguous-habitat-work.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "#set up logging \n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "debug=logging.debug\n",
    "info=logging.info\n",
    "warning=logging.warning\n",
    "error=logging.error\n",
    "\n",
    "gdb = r\"\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\ContiguousHabitat_2025-06-25.gdb\"\n",
    "tempgdb = r\"\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\TEMP_ContiguousHabitat.gdb\"\n",
    "\n",
    "studyArea = r\"\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\WMB_Study_Area_2024_07_30\\WMB_Study_Area_2024_07_30.shp\"\n",
    "privateLand = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\ownership\\iflb_own_studyArea.shp'\n",
    "vri_gdb = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\Disturbance products for BRFN - Copy\\data\\BRFN_FOREST_DSTRB.gdb'\n",
    "\n",
    "disturbance_dir = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\Disturbance'\n",
    "\n",
    "workspace = r'C:\\Users\\NROSS\\OneDrive - Government of BC\\Documents\\Projects\\WMBTesting\\wmb-duckdb'\n",
    "work_db = os.path.join(workspace, \"contiguous-habitat-work.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x2b281f83c30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to duckdb\n",
    "con = duckdb.connect(work_db)\n",
    "con.execute(\"INSTALL spatial; LOAD spatial;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read study area and private land\n",
    "\n",
    "study_gdf = gpd.read_file(studyArea)\n",
    "study_gdf['wkt'] = study_gdf['geometry'].to_wkt()\n",
    "study_gdf = study_gdf.drop(columns='geometry')\n",
    "con.sql(\"CREATE TABLE IF NOT EXISTS studyarea AS (SELECT *,  ST_GeomFromText(wkt) as GEOM FROM study_gdf)\")\n",
    "\n",
    "private_gdf = gpd.read_file(privateLand)\n",
    "private_gdf['wkt'] = private_gdf['geometry'].to_wkt()\n",
    "private_gdf = private_gdf.drop(columns='geometry')\n",
    "con.sql(\"CREATE TABLE IF NOT EXISTS privateland AS (SELECT *,  ST_GeomFromText(wkt) as GEOM FROM private_gdf)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fc_to_duckdb(gdb, layer, src_name, clause, duckdbTable, broadBuffer, refinedBuffer):\n",
    "    \"\"\"Reads feature class or shapefile to geopandas, removes all fields except the layer name, adds appropriate buffer fields, converts to duckdb and inserts into a table\n",
    "\n",
    "    Args:\n",
    "        gdb (str): path to geodatabase\n",
    "        layer (str): layer (feature class) name. Leave None for shapefile or to pick the first layer\n",
    "        src_name (str): Name to list in the source column in the duckdb table\n",
    "        clause (str): SQL clause when reading the input data\n",
    "        duckdbTable (str): Name of existing table in duckdb\n",
    "        broadBuffer (int): Buffer distance for broad contiguous habitat. Set to None to ignore for this category.\n",
    "        refinedBuffer (int): Buffer distance for refined contiguous habitat\n",
    "    \"\"\"\n",
    "    info(f\"Reading {src_name} to geopandas with clause {clause}\")\n",
    "    gdf = gpd.read_file(gdb, layer=layer, where=clause)\n",
    "    info(f'\\tContains {len(gdf)} records')\n",
    "    gdf['wkt'] = gdf['geometry'].to_wkt()\n",
    "    gdf = gdf.drop(columns='geometry')\n",
    "    gdf['source'] = src_name\n",
    "    gdf['broadbuffer'] = broadBuffer\n",
    "    gdf['refinedbuffer'] = refinedBuffer\n",
    "    \n",
    "    info(f\"\\t inserting into {duckdbTable}\")\n",
    "    con.sql(f\"INSERT INTO {duckdbTable} BY NAME (SELECT ST_GeomFromText(gdf.wkt) as GEOM, source, broadbuffer, refinedbuffer FROM gdf JOIN studyarea s ON ST_Intersects(ST_GeomFromText(gdf.wkt), s.GEOM))\")\n",
    "    del gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:fiona._env:GDAL_DATA found in environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:fiona._env:PROJ_DATA found in environment.\n",
      "INFO:root:Reading Applications_inReview_19Sept2024 to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading FTML_WOODLOTS_AandB_activeonly_deferredRemoved to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading PASR_Assoc_Ancillary_poly to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading PASR_Pipeline_poly to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading PASR_Road_poly to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading PASR_WellFacility_Pads_poly to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading Pipeline_ROWs_Infill_2024_12_04 to geopandas with clause \n",
      "c:\\Users\\NROSS\\.conda\\envs\\pyduckdb\\Lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: organizePolygons() received a polygon with more than 100 parts. The processing may be really slow.  You can skip the processing by setting METHOD=SKIP, or only make it analyze counter-clock wise parts by setting METHOD=ONLY_CCW if you can assume that the outline of holes is counter-clock wise defined\n",
      "  return ogr_read(\n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_AG to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_Com to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_Mine to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_OG to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_POW to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_Rail to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_Rec to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_Road to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading RSEA_Trail to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading Schedule_O_part1 to geopandas with clause \n",
      "c:\\Users\\NROSS\\.conda\\envs\\pyduckdb\\Lib\\site-packages\\pyogrio\\raw.py:198: UserWarning: Measured (M) geometry types are not supported. Original type 'Measured 3D MultiPolygon' is converted to 'MultiPolygon Z'\n",
      "  return ogr_read(\n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading Schedule_O_part2 to geopandas with clause \n",
      "c:\\Users\\NROSS\\.conda\\envs\\pyduckdb\\Lib\\site-packages\\pyogrio\\raw.py:198: UserWarning: Measured (M) geometry types are not supported. Original type 'Measured 3D MultiPolygon' is converted to 'MultiPolygon Z'\n",
      "  return ogr_read(\n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading ScheduleK_Blocks to geopandas with clause \n",
      "c:\\Users\\NROSS\\.conda\\envs\\pyduckdb\\Lib\\site-packages\\pyogrio\\raw.py:198: UserWarning: Measured (M) geometry types are not supported. Original type 'Measured 3D MultiPolygon' is converted to 'MultiPolygon Z'\n",
      "  return ogr_read(\n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading Seismic_BCER_1996_2004_WMBcleaned_poly to geopandas with clause CutType = 'Mech'\n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading Seismic_BCER_2002_2006_WMBcleaned_poly to geopandas with clause CutType = 'Mech'\n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading Seismic_BCER_Legacy2D_WMBcleaned_poly to geopandas with clause \n"
     ]
    }
   ],
   "source": [
    "# Create all_dist table and add W2W layers\n",
    "\n",
    "con.sql(\"DROP TABLE IF EXISTS all_dist\")\n",
    "con.sql(\"CREATE SEQUENCE IF NOT EXISTS seq_distid START 1;\")\n",
    "con.sql(\"CREATE TABLE all_dist (id INTEGER PRIMARY KEY DEFAULT nextval('seq_distid'), geom GEOMETRY, source VARCHAR, broadbuffer FLOAT, refinedbuffer FLOAT)\")\n",
    "\n",
    "w2w_gdb = os.path.join(disturbance_dir, 'Wall_To_Wall_2025_06_05.gdb')\n",
    "\n",
    "# list disturbance buffer categories\n",
    "broad_standard = 50\n",
    "refined_standard = 100\n",
    "broad_noisy = 50\n",
    "refined_noisy = 500\n",
    "refined_seismic_mech = 50\n",
    "refined_seismic_legacy = 100\n",
    "\n",
    "def get_w2w_buffers_dict(fc):\n",
    "    \n",
    "    outDict = {}\n",
    "        \n",
    "    if fc in ['RSEA_Seismic', 'RSEA_Fire', 'RSEA_Cutblocks']: # add any layers to ignore to this list\n",
    "        outDict['broadbuffer'] = None\n",
    "        outDict['refinedbuffer'] = None\n",
    "    elif fc == \"RSEA_AG\":\n",
    "        outDict['broadbuffer'] = 0\n",
    "        outDict['refinedbuffer'] = 0\n",
    "    elif fc == 'RSEA_Rail':\n",
    "        outDict['broadbuffer'] = broad_noisy\n",
    "        outDict['refinedbuffer'] = refined_noisy\n",
    "        # the other \"noisy\" datasets are outside the w2w\n",
    "        \n",
    "    elif \"Seismic_\" in fc:\n",
    "        outDict['broadbuffer'] = None\n",
    "        if fc =='Seismic_BCER_Legacy2D_WMBcleaned_poly':\n",
    "            outDict['refinedbuffer'] = refined_seismic_legacy\n",
    "        elif fc in ['Seismic_BCER_1996_2004_WMBcleaned_poly', 'Seismic_BCER_2002_2006_WMBcleaned_poly', 'Seismic_BCER_Permitted_WMBcleaned_poly']:\n",
    "            outDict['refinedbuffer'] = refined_seismic_mech\n",
    "            \n",
    "            # Note - these need all lines except CutType = \"Mech\" removed\n",
    "            outDict['clause'] = \"CutType = 'Mech'\"\n",
    "            \n",
    "    else: # set all others to the standard buffer distance\n",
    "        outDict['broadbuffer'] = broad_standard\n",
    "        outDict['refinedbuffer'] = refined_standard\n",
    "    \n",
    "    if 'clause' not in outDict:\n",
    "        outDict['clause'] = ''\n",
    "    \n",
    "    return outDict\n",
    "\n",
    "for fc in listlayers(w2w_gdb):\n",
    "    # Ignore the \"line\" features and a few RSEA features\n",
    "    if '_line' in fc or fc in ['RSEA_Seismic', 'RSEA_Fire', 'RSEA_Cutblocks']:\n",
    "        continue\n",
    "    \n",
    "    # Get buffer dictionary\n",
    "    bufferDict = get_w2w_buffers_dict(fc)\n",
    "    \n",
    "    # insert into duckdb 'all_dist' table\n",
    "    read_fc_to_duckdb(w2w_gdb, fc, fc, bufferDict['clause'], 'all_dist', bufferDict['broadbuffer'], bufferDict['refinedbuffer'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading VRI Cutblocks to geopandas with clause DSTRB_HIST LIKE '%CUT%' And MRSRD_Y > 1984\n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading Compressors_Plants_wShape.shp to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n",
      "INFO:root:Reading MainRoads_2024_12_04_Footprint.shp to geopandas with clause \n",
      "INFO:root:\t inserting into all_dist\n"
     ]
    }
   ],
   "source": [
    "# Add VRI cutblocks\n",
    "read_fc_to_duckdb(\n",
    "    vri_gdb, \n",
    "    layer = 'merged', \n",
    "    src_name = 'VRI Cutblocks',\n",
    "    clause = \"DSTRB_HIST LIKE '%CUT%' And MRSRD_Y > 1984\", \n",
    "    duckdbTable='all_dist', \n",
    "    broadBuffer = broad_standard, \n",
    "    refinedBuffer = refined_standard\n",
    "    )\n",
    "\n",
    "# Add noisy disturbances\n",
    "noisy_dist_dir = os.path.join(disturbance_dir, 'NoisyDisturbances', 'Updates_forNorth')\n",
    "for shp in ['Compressors_Plants_wShape.shp', 'MainRoads_2024_12_04_Footprint.shp']:\n",
    "    read_fc_to_duckdb(\n",
    "        os.path.join(noisy_dist_dir, shp), \n",
    "        layer = None, \n",
    "        src_name = shp,\n",
    "        clause = '', \n",
    "        duckdbTable='all_dist', \n",
    "        broadBuffer = broad_noisy, \n",
    "        refinedBuffer = refined_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>table_catalog</th>\n",
       "            <th>table_schema</th>\n",
       "            <th>table_name</th>\n",
       "            <th>table_type</th>\n",
       "            <th>self_referencing_column_name</th>\n",
       "            <th>reference_generation</th>\n",
       "            <th>user_defined_type_catalog</th>\n",
       "            <th>user_defined_type_schema</th>\n",
       "            <th>user_defined_type_name</th>\n",
       "            <th>is_insertable_into</th>\n",
       "            <th>is_typed</th>\n",
       "            <th>commit_action</th>\n",
       "            <th>TABLE_COMMENT</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>contiguous-habitat-work</td>\n",
       "            <td>main</td>\n",
       "            <td>all_dist</td>\n",
       "            <td>BASE TABLE</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>YES</td>\n",
       "            <td>NO</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>contiguous-habitat-work</td>\n",
       "            <td>main</td>\n",
       "            <td>privateland</td>\n",
       "            <td>BASE TABLE</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>YES</td>\n",
       "            <td>NO</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>contiguous-habitat-work</td>\n",
       "            <td>main</td>\n",
       "            <td>studyarea</td>\n",
       "            <td>BASE TABLE</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "            <td>YES</td>\n",
       "            <td>NO</td>\n",
       "            <td>None</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------------------+--------------+-------------+------------+------------------------------+----------------------+---------------------------+--------------------------+------------------------+--------------------+----------+---------------+---------------+\n",
       "|      table_catalog      | table_schema |  table_name | table_type | self_referencing_column_name | reference_generation | user_defined_type_catalog | user_defined_type_schema | user_defined_type_name | is_insertable_into | is_typed | commit_action | TABLE_COMMENT |\n",
       "+-------------------------+--------------+-------------+------------+------------------------------+----------------------+---------------------------+--------------------------+------------------------+--------------------+----------+---------------+---------------+\n",
       "| contiguous-habitat-work |     main     |   all_dist  | BASE TABLE |             None             |         None         |            None           |           None           |          None          |        YES         |    NO    |      None     |      None     |\n",
       "| contiguous-habitat-work |     main     | privateland | BASE TABLE |             None             |         None         |            None           |           None           |          None          |        YES         |    NO    |      None     |      None     |\n",
       "| contiguous-habitat-work |     main     |  studyarea  | BASE TABLE |             None             |         None         |            None           |           None           |          None          |        YES         |    NO    |      None     |      None     |\n",
       "+-------------------------+--------------+-------------+------------+------------------------------+----------------------+---------------------------+--------------------------+------------------------+--------------------+----------+---------------+---------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup sql magic and show current tables\n",
    "%sql con --alias duckdb\n",
    "%sql select * from information_schema.tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Success</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+\n",
       "| Success |\n",
       "+---------+\n",
       "+---------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dist_buffer_broad;\n",
    "DROP TABLE IF EXISTS dist_buffer_refined;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Count</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1327204</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+\n",
       "|  Count  |\n",
       "+---------+\n",
       "| 1327204 |\n",
       "+---------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dist_buffer_broad AS (\n",
    "    SELECT id, source, ST_Simplify(ST_Buffer(geom, broadbuffer), 10) as geom FROM all_dist WHERE broadbuffer IS NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dist_buffer_refined AS (\n",
    "    SELECT id, source, ST_Simplify(ST_Buffer(geom, refinedbuffer), 10) as geom FROM all_dist WHERE refinedbuffer IS NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dist_buffer_refined_50 AS (\n",
    "    SELECT id, source, ST_Simplify(ST_Buffer(geom, 50), 10) as geom FROM all_dist WHERE refinedbuffer IS NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duckdb_to_gdb(table_name):\n",
    "    gdf = con.sql(f\"SELECT id, source, ST_AsText(GEOM) as wkt from {table_name}\").to_df()\n",
    "    gdf['geometry'] = gpd.GeoSeries.from_wkt(gdf['geometry'])\n",
    "    gdf = gpd.GeoDataFrame(gdf)\n",
    "    gdf.to_file(tempgdb, layer=table_name)\n",
    "\n",
    "duckdb_to_gdb('dist_buffer_broad')\n",
    "duckdb_to_gdb('dist_buffer_refined')\n",
    "duckdb_to_gdb('dist_buffer_refined_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to arcpy for the last part due to duckdb issues with processing this many verticies\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tempdir = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\MarxanInputs\\Contiguous'\n",
    "gdb = os.path.join(tempdir, \"ContiguousHabitat_20250625.gdb\")\n",
    "tempgdb = os.path.join(tempdir, \"TEMP_ContiguousHabitat.gdb\")\n",
    "\n",
    "studyArea = r\"\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\WMB_Study_Area_2024_07_30\\WMB_Study_Area_2024_07_30.shp\"\n",
    "privateLand = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\ownership\\private_40_41_studyArea.shp'\n",
    "vri_gdb = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\Disturbance products for BRFN - Copy\\data\\BRFN_FOREST_DSTRB.gdb'\n",
    "wmb = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Project\\WMB_Planning.gdb\\StudyArea_Erase' # Note that this data is the study area split by the closest WMB\n",
    "buffer_geopackage = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\MarxanInputs\\Contiguous\\contiguous_2025-06-26.gpkg'\n",
    "\n",
    "disturbance_dir = r'\\\\spatialfiles.bcgov\\work\\srm\\fsj\\Workarea\\nross\\WMBPlanning\\Data\\Disturbance'\n",
    "\n",
    "arcpy.env.workspace = tempgdb\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# for distType in ['broad', 'refined']:\n",
    "for distType in ['refined']:\n",
    "    # erase the buffered disturbance from the study area\n",
    "    print(f\"erasing {distType} in {tempgdb}\")\n",
    "    arcpy.analysis.Erase(\n",
    "        in_features=studyArea,\n",
    "        erase_features=f\"dist_buffer_{distType}_dissolve\",\n",
    "        out_feature_class=f\"interior_{distType}_1\",\n",
    "        cluster_tolerance=None\n",
    "    )\n",
    "\n",
    "    # erase the private land from this\n",
    "    print(f\"erasing private land from {distType}\")\n",
    "    arcpy.analysis.Erase(\n",
    "        in_features=f\"interior_{distType}_1\",\n",
    "        erase_features=privateLand,\n",
    "        out_feature_class=f\"interior_{distType}_noPrivate\",\n",
    "        cluster_tolerance=None\n",
    "    )\n",
    "    # explode to singlepart\n",
    "    print(f\"explode {distType}\")\n",
    "    arcpy.management.MultipartToSinglepart(\n",
    "        in_features=f\"interior_{distType}_noPrivate\",\n",
    "        out_feature_class=f\"interior_{distType}_noPrivate_exploded\"\n",
    "    )\n",
    "\n",
    "    # Calculate hectares\n",
    "    print(f\"calculate area {distType}\")\n",
    "    arcpy.management.CalculateField(\n",
    "        in_table=f\"interior_{distType}_noPrivate_exploded\",\n",
    "        field=\"InteriorAreaHa\",\n",
    "        field_type=\"DOUBLE\",\n",
    "        expression=\"!Shape_Area!/10000\"\n",
    "    )\n",
    "    # Intersect with WMB\n",
    "    print(f\"WMB intersect {distType}\")\n",
    "    arcpy.analysis.Identity(\n",
    "        in_features=f\"interior_{distType}_noPrivate_exploded\",\n",
    "        identity_features=wmb,\n",
    "        out_feature_class=f\"interior_{distType}_noPrivate_exploded_byWMB\",\n",
    "        join_attributes=\"ALL\",\n",
    "        cluster_tolerance=None,\n",
    "        relationship=\"NO_RELATIONSHIPS\"\n",
    "    )\n",
    "\n",
    "    # get the Threshold values from Pandas\n",
    "    # Convert feature class to NumPy array then pandas dataframe\n",
    "    numpy_array = arcpy.da.FeatureClassToNumPyArray(f\"interior_{distType}_noPrivate_exploded_byWMB\", ['OBJECTID', 'WMB', 'Inner', 'Shape_Area'])\n",
    "    df = pd.DataFrame(numpy_array)\n",
    "\n",
    "    df['AreaHa'] = df['Shape_Area']/10000\n",
    "    sorteddf = df.loc[df['Inner']=='Inner'].sort_values('AreaHa', ascending=False)\n",
    "    sorteddf['Cumul'] = sorteddf.groupby([\"WMB\"])['AreaHa'].transform(pd.Series.cumsum) \n",
    "    # Get the sum of all patches by WMB\n",
    "    sorteddf['WMBInteriorTotal'] = sorteddf.groupby([\"WMB\"])['AreaHa'].transform(pd.Series.sum)\n",
    "    # Get percentage of each cumulative step of the entire df\n",
    "    sorteddf['Interior_Cumulative_Entire_pct'] = sorteddf['Cumul'] / sorteddf['WMBInteriorTotal']\n",
    "\n",
    "    # Create summary tables showing the last (meaning maximum in each group) polygon area and OBJECTID before reaching the 25% and 50% values for the entire area.\n",
    "    p25 = sorteddf.loc[sorteddf['Interior_Cumulative_Entire_pct'] <= 0.25][[\"WMB\", 'AreaHa', 'OBJECTID']].groupby([\"WMB\"]).last()\n",
    "    p50 = sorteddf.loc[sorteddf['Interior_Cumulative_Entire_pct'] <= 0.5][[\"WMB\",'AreaHa', 'OBJECTID', 'Cumul']].groupby([\"WMB\"]).last()\n",
    "\n",
    "    # Join these two together into one threshold dataframe and display\n",
    "    thresh_df = p25.join(p50, lsuffix=\"_25\", rsuffix=\"_50\").reset_index()\n",
    "    print(thresh_df)\n",
    "    df = df.merge(thresh_df[['WMB', 'AreaHa_25', 'AreaHa_50']], on='WMB')\n",
    "    df.loc[df['AreaHa'] >= df['AreaHa_50'], 'threshold'] = 50.0\n",
    "    df.loc[df['AreaHa'] >= df['AreaHa_25'], 'threshold'] = 25.0\n",
    "\n",
    "    # Export this table to a csv and join the field back on to the Interior area\n",
    "    df[['OBJECTID', 'WMB', 'threshold']].to_csv(os.path.join(tempdir, f'interior_{distType}_threshold.csv'))\n",
    "\n",
    "    print(\"Joining threshold field to interior\")\n",
    "    # JOIN FIELD to the interior on OBJECTID\n",
    "    arcpy.management.JoinField(\n",
    "        in_data=f\"interior_{distType}_noPrivate_exploded_byWMB\",\n",
    "        in_field=\"OBJECTID\",\n",
    "        join_table=os.path.join(tempdir, f'interior_{distType}_threshold.csv'),\n",
    "        join_field=\"OBJECTID\",\n",
    "        fields=\"threshold\",\n",
    "        fm_option=\"NOT_USE_FM\",\n",
    "        field_mapping=None,\n",
    "        index_join_fields=\"NO_INDEXES\"\n",
    "    )\n",
    "\n",
    "    # export this as the Interior layer\n",
    "    arcpy.conversion.FeatureClassToFeatureClass(\n",
    "        f\"interior_{distType}_noPrivate_exploded_byWMB\",\n",
    "        out_path=gdb, out_name=f\"interior_{distType}\"\n",
    "    )\n",
    "    if distType == 'broad':\n",
    "\n",
    "        # Buffer out again\n",
    "        print(\"Buffering again\")\n",
    "        arcpy.analysis.Buffer(\n",
    "            in_features=f\"interior_{distType}_noPrivate_exploded_byWMB\",\n",
    "            out_feature_class=f\"contiguous_{distType}_buffered\",\n",
    "            buffer_distance_or_field=50\n",
    "        )\n",
    "\n",
    "        # erase private again\n",
    "        print(\"erase private again\")\n",
    "        arcpy.analysis.Erase(\n",
    "            in_features=f\"contiguous_{distType}_buffered\",\n",
    "            erase_features=privateLand,\n",
    "            out_feature_class=os.path.join(gdb, f\"Contiguous_{distType}\"),\n",
    "            cluster_tolerance=None\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Take the Refined dist buffered by 50m\n",
    "        # Erase this from the study area and explode\n",
    "        print(f\"Erasing disturbance refined from study area\")\n",
    "        arcpy.analysis.Erase(\n",
    "            in_features=studyArea,\n",
    "            erase_features=f\"dist_buffer_{distType}_50_dissolve\",\n",
    "            out_feature_class=f\"interior_{distType}_50\",\n",
    "            cluster_tolerance=None\n",
    "        )\n",
    "        # explode to singlepart\n",
    "        print(\"explode\")\n",
    "        arcpy.management.MultipartToSinglepart(\n",
    "            in_features=f\"interior_{distType}_50\",\n",
    "            out_feature_class=f\"interior_{distType}_50_exploded\"\n",
    "        )\n",
    "\n",
    "        # buffer each of these by 50m \n",
    "        print(\"Buffer these by 50\")\n",
    "        arcpy.analysis.Buffer(\n",
    "            in_features=f\"interior_{distType}_50_exploded\",\n",
    "            out_feature_class=f\"contiguous_{distType}_1\",\n",
    "            buffer_distance_or_field=50\n",
    "        )\n",
    "\n",
    "        # erase private again\n",
    "        print(\"erase private again\")\n",
    "        arcpy.analysis.Erase(\n",
    "            in_features=f\"contiguous_{distType}_1\",\n",
    "            erase_features=privateLand,\n",
    "            out_feature_class=f\"contiguous_{distType}_2\",\n",
    "            cluster_tolerance=None\n",
    "        )\n",
    "        # Use summarize within to get the sum, mean, max and count of refined interior are within\n",
    "        print(\"summarize refined interior within these areas\")\n",
    "        arcpy.management.CalculateField(f\"interior_{distType}_noPrivate_exploded_byWMB\", \"threshold_int\", \"int(!threshold!)\", field_type = \"SHORT\")\n",
    "        arcpy.analysis.SummarizeWithin(\n",
    "            in_polygons=f\"contiguous_{distType}_2\",\n",
    "            in_sum_features=f\"interior_{distType}_noPrivate_exploded_byWMB\",\n",
    "            out_feature_class=os.path.join(gdb, f\"Contiguous_{distType}\"),\n",
    "            keep_all_polygons=\"KEEP_ALL\",\n",
    "            sum_fields=\"InteriorAreaHa Max;InteriorAreaHa Mean;InteriorAreaHa Sum; threshold_int Min\",\n",
    "            sum_shape=\"NO_SHAPE_SUM\",\n",
    "            shape_unit=\"HECTARES\",\n",
    "            group_field=None,\n",
    "            add_min_maj=\"NO_MIN_MAJ\",\n",
    "            add_group_percent=\"NO_PERCENT\",\n",
    "            out_group_table=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer these by 50\n",
      "summarize refined interior within these areas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2 class='msg-title'>Messages</h2><div id='messages'>Start Time: June 27, 2025 4:36:19 PM<br>Succeeded at June 27, 2025 4:45:50 PM (Elapsed Time: 9 minutes 31 seconds)<br></div><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result '\\\\\\\\spatialfiles.bcgov\\\\work\\\\srm\\\\fsj\\\\Workarea\\\\nross\\\\WMBPlanning\\\\Data\\\\MarxanInputs\\\\Contiguous\\\\ContiguousHabitat_20250625.gdb\\\\Contiguous_refined'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# buffer each of these by 50m \n",
    "print(\"Buffer these by 50\")\n",
    "# arcpy.analysis.Buffer(\n",
    "#     in_features=f\"interior_{distType}_50_exploded\",\n",
    "#     out_feature_class=f\"contiguous_{distType}_1\",\n",
    "#     buffer_distance_or_field=50\n",
    "# )\n",
    "\n",
    "\n",
    "# Use summarize within to get the sum, mean, max and count of refined interior are within\n",
    "print(\"summarize refined interior within these areas\")\n",
    "arcpy.analysis.SummarizeWithin(\n",
    "    in_polygons=f\"contiguous_{distType}_1\",\n",
    "    in_sum_features=f\"interior_{distType}_noPrivate_exploded_byWMB\",\n",
    "    out_feature_class=os.path.join(gdb, f\"Contiguous_{distType}\"),\n",
    "    keep_all_polygons=\"KEEP_ALL\",\n",
    "    sum_fields=\"InteriorAreaHa Max;InteriorAreaHa Mean;InteriorAreaHa Sum; threshold_int Min\",\n",
    "    sum_shape=\"NO_SHAPE_SUM\",\n",
    "    shape_unit=\"HECTARES\",\n",
    "    group_field=None,\n",
    "    add_min_maj=\"NO_MIN_MAJ\",\n",
    "    add_group_percent=\"NO_PERCENT\",\n",
    "    out_group_table=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
